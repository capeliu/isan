{"note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Isan","tagline":"Chinese processing","body":"isan\r\n====\r\n\r\n> “举一隅不以三隅反，则不复也” ——《论语·述而》\r\n\r\n一个数据驱动的中文处理**实验环境**，可进行**中文分词**（以后会实现 *词性标注* 、 *句法分析* ）。作者[张开旭](http://weibo.com/zhangkaixu)。\r\n\r\n特点\r\n\r\n* 速度**慢**，不为速度牺牲性能，同时保证算法的各个组件的代码容易修改，但理论上保持`O(n)`的时间复杂度。\r\n* 效果**差**，本系统专为实验使用，目标并非实用分词工具，处理真实文本效果并不理想。但可用于设计实用分词系统。\r\n* 使用**难**，虽然不太实用，但有很多“手动功能”，可以通过理解并修改源代码来完成算法和设置的充分修改。\r\n\r\n# Run\r\n\r\n## Getting started\r\n\r\n在此以Ubuntu操作系统为例，介绍如何安装和使用**isan**。\r\n\r\n首先，需要安装必要的软件包，包括gcc，make，Python3，Python3-dev和git，在命令行下安装：\r\n\r\n    sudo apt-get install gcc\r\n    sudo apt-get install make\r\n    sudo apt-get install python3\r\n    sudo apt-get install python3-dev\r\n    sudo apt-get install git\r\n\r\n选好路径，使用git下载**isan**源代码，编译。\r\n\r\n    git clone https://github.com/zhangkaixu/isan.git\r\n    cd isan\r\n    make\r\n\r\n下载一个可供实验用的SIGHAN05中文分词语料库。\r\n\r\n    wget http://www.sighan.org/bakeoff2005/data/icwb2-data.rar\r\n    sudo apt-get install unrar\r\n    mkdir sighan05\r\n    unrar e icwb2-data.rar sighan05\r\n    \r\n试着训练和测试，看看程序是否安装正确。\r\n    \r\n    ./cws.sh test.bin --train sighan05/msr_test_gold.utf8\r\n    ./cws.sh test.bin --test sighan05/msr_test_gold.utf8\r\n    \r\n如果以上一切顺利，将会看到测试结果能有0.99以上的F1值。接下来就可以试着真枪实弹地来一次，在MSR的训练集上迭代20次训练模型，每次迭代都将测试集作为开发集检查一下模型性能。\r\n    \r\n    ./cws.sh test.bin --train sighan05/msr_training.utf8 --iteration=20 --dev sighan05/msr_test_gold.utf8 --beam_width=16\r\n    \r\n可以看到最后效果保持在0.973左右，一个还算可以的baseline吧？\r\n\r\n那么只要对代码的某些地方进行进一步的修改，就可能有比baseline更好的分词模型了。\r\n\r\n## Command line\r\n\r\n以中文分词为例，命令行用法如下：\r\n\r\n    ./cws.sh model_file [--train training_file] [--test test_file] [--dev dev_file]\r\n        [--iteration iter] [--beam_width w]\r\n\r\n其中：\r\n\r\n* `model_file` 是用以存储模型参数的文件\r\n* `training_file` 如果给出，会使用该文件训练模型\r\n* `dev_file` 如果给出，会在每次训练迭代之后在`dev_file`上测试模型效果\r\n* `test_file` 如果给出，会使用已有模型或新训练的模型在该文件上进行测试\r\n* 如果 `training_file` 和 `test_file` 均未给出，则对标准输入流中的文本进行分词，输出结果到标准输出流\r\n* `iter` 用来指定训练时的迭代次数\r\n* `w` 用来指定柱搜索解码中的搜索宽度（柱宽度）\r\n\r\n词性标注的基本命令行用法相同，只需要将`cws.sh`替换为`tag.py`（当然部分功能尚未实现）。\r\n\r\n## Input & output\r\n\r\n每个句子一行\r\n\r\n* 原始句子 `材料利用率高。`\r\n* 分词结果 `材料 利用率 高 。`\r\n* 词性标注结果 `材料_NN 利用率_NN 高_VA 。_PU`\r\n\r\n例如在分词`./cws.sh`命令中，`training_file` 、 `dev_file`、 `test_file` 以及预测阶段的输出的格式均是使用的分词结果格式， 预测阶段的输入格式是原始句子格式。\r\n\r\n\r\n# Architecture\r\n\r\n现以分词模型为例，介绍程序架构。\r\n\r\n## The program\r\n\r\n首先看看`./cws.sh`文件，其包含了模型的三个组成部分：\r\n\r\n    ./isan.py \\\r\n        --model isan.common.perceptrons.Model \\\r\n        --decoder isan.common.decoder.DFA \\\r\n        --task isan.tagging.cws.Task \\\r\n        $*\r\n    \r\n首先是`--task`，包含了分词相关的所有代码，仔细阅读[源代码](https://github.com/zhangkaixu/isan/blob/master/isan/tagging/default_segger_c.py)中的所有内容，修改其中的代码，就能DIY出自己的分词模型。\r\n\r\n其次是`--decoder`，是一个解码器，与具体的任务无关。分词使用的是基于状态转移的`DFA`解码器，也就是一个维特比解码器，相同解码器也可完成词性标注等任务。此外模型的特征权重也由Searcher管理。\r\n\r\n最后是`--model`，是控制模型学习、预测过程的，与具体的任务和解码器均无关。该例子中使用的是平均感知器算法进行模型的学习和预测。\r\n\r\n## Decoding\r\n\r\n解码就是根据**输入**搜索得到**输出**的过程。\r\n\r\n### Actions\r\n\r\n**isan**是这样建模的。首先定义一个**动作**的集合，解码过程就是根据输入，得到一个动作序列，然后根据输入与动作序列，就能唯一确定输出。因此解码的过程就是根据输入得到动作序列的过程。\r\n\r\n在分词例子中，动作有两种，即 *分* 与 *断* ，一个有`n`个字的输入，对应的动作序列中有`n+1`个动作，分别表示各个字边界是分还是断。\r\n\r\n### Status\r\n\r\n下面的问题是如何依次确定动作序列中每个位置具体的动作。首先引入**状态**这一概念，有一个初始状态，一个状态根据不同的动作转移到下一个不同的状态。最后到终止状态。**isan**根据当前状态确定下一个动作哪个好哪个不好。\r\n\r\n### Features\r\n\r\n具体地，由状态和输入可生成每个状态的特征，每个特征-动作二元组对应一个权重。那么对于一个动作，一个状态的`k`个特征就有`k`个权重，它们线性相加就是一个分数，对应着这个动作的优劣。\r\n\r\n接下来，回到最开始的解码中确定动作序列的问题上，一个动作序列的的好坏，由序列中每个动作的分数相加得到。解码就是尽量找到分数最高的动作序列，以此生成输出。\r\n","google":"UA-33732606-2"}